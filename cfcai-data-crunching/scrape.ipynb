{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_url(url):\n",
    "    try:\n",
    "        return requests.get(url, timeout=2).ok\n",
    "    except requests.exceptions.ReadTimeout:\n",
    "        return False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return False\n",
    "    except requests.exceptions.TooManyRedirects:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "newspaper_config = newspaper.Config()\n",
    "newspaper_config.fetch_images = False\n",
    "newspaper_config.request_timeout = 2\n",
    "newspaper_config.memoize_articles = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "newspaper_config.verbose = 1\n",
    "\n",
    "url = 'https://www.who.int/emergencies/diseases/novel-coronavirus-2019'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newspaper_obj = newspaper.build(url, config=newspaper_config, request_timeout=3, number_threads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article(article):\n",
    "    article.download()\n",
    "    html = article.html\n",
    "    article.parse()\n",
    "    article_data = {}\n",
    "    article.url = article.url.strip()\n",
    "    \n",
    "    article_data['title'] = article.title\n",
    "    article_data['text'] = article.text\n",
    "    article_data['url'] = article.url\n",
    "    article_data['html'] = html\n",
    "    return article_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# art_data = get_article(newspaper_obj.articles[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11'\n",
    "            }\n",
    "def get_html(url):\n",
    "    url_lower = url.lower()\n",
    "    for ending in ['pdf', 'jpg', 'jpeg', 'png']:\n",
    "        if url_lower.endswith(ending):\n",
    "            return ''\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=5.0)\n",
    "    except:\n",
    "        return ''\n",
    "    if response.status_code != 200:\n",
    "        return ''\n",
    "    if 'text/html' not in response.headers['content-type']:\n",
    "#         print(response.headers['content-type'])\n",
    "        return ''\n",
    "    html = response.text\n",
    "    if html.startswith('%PDF'):\n",
    "        return ''\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def get_domain(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = '{uri.netloc}'.format(uri=parsed_url)\n",
    "    return domain\n",
    "\n",
    "\n",
    "def scrape_sub_urls(url):\n",
    "    html = get_html(url)\n",
    "    soup = BeautifulSoup(html)\n",
    "    my_domain = get_domain(url)\n",
    "    links = [link.attrs['href'] for link in soup(['a']) if 'href' in link.attrs and get_domain(link.attrs['href']) == my_domain]\n",
    "    return set(links), html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_urls(root_url, max_depth=3):\n",
    "    all_urls = {}\n",
    "    def scrape_recursive(url, all_urls, depth):\n",
    "        if depth > max_depth:\n",
    "            return \n",
    "        sub_urls, html = scrape_sub_urls(url)\n",
    "        all_urls[url] = html\n",
    "        new_urls = sub_urls.difference(all_urls.keys())\n",
    "        all_urls.update(dict.fromkeys(new_urls))\n",
    "#         print(url, len(new_urls))\n",
    "        for u in new_urls:\n",
    "            scrape_recursive(u, all_urls, depth+1)\n",
    "            \n",
    "    all_urls[root_url] = None\n",
    "    scrape_recursive(root_url, all_urls, 0)\n",
    "    \n",
    "    n_leafs = sum([html is None for html in all_urls.values()])\n",
    "    print('downloading %d leaf nodes' % n_leafs)\n",
    "    for url, html in all_urls.items():\n",
    "        if all_urls[url] is None:\n",
    "            all_urls[url] = get_html(url)\n",
    "    return all_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def crawler_urls(url, max_depth=1):\n",
    "    urls = scrape_urls(url, max_depth=max_depth)\n",
    "    articles = {}\n",
    "    for u, html in tqdm(urls.items()):\n",
    "        art = newspaper.Article(url)\n",
    "        if html == '' or html is None:\n",
    "            continue\n",
    "        try:\n",
    "            art.download(input_html=html)\n",
    "            art.parse()\n",
    "            art.nlp()\n",
    "            articles[u] = art\n",
    "        except:\n",
    "            print(u)\n",
    "            pass\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def crawler_newspaper(root_url, max_depth=5):\n",
    "    all_articles = {}\n",
    "    def crawl(url, all_articles, depth):\n",
    "        my_domain = get_domain(url)\n",
    "        if depth > max_depth:\n",
    "            return\n",
    "        newspaper_obj = newspaper.build(url, config=newspaper_config, request_timeout=8, number_threads=4)\n",
    "        \n",
    "        new_dict = dict(zip(newspaper_obj.article_urls(), newspaper_obj.articles))\n",
    "        new_dict = {u: a for u, a in new_dict.items() if get_domain(url) == crawl}\n",
    "        new_keys = set(new_dict.keys()).difference(all_articles)\n",
    "        print(url, len(newspaper_obj.articles), len(new_keys))\n",
    "#         if depth >= 1:\n",
    "#             print(new_keys)\n",
    "        \n",
    "        all_articles.update(new_dict)\n",
    "#         print(url, len(newspaper_obj.articles))\n",
    "        for u in newspaper_obj.article_urls():\n",
    "            if u in new_keys:\n",
    "                crawl(u, all_articles, depth+1)\n",
    "                \n",
    "        \n",
    "    crawl(root_url, all_articles, 0)\n",
    "    \n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_articles(url, max_depth=3):\n",
    "    print('scraping by url')\n",
    "    articles_urls = crawler_urls(url, max_depth=max_depth)\n",
    "    print('scraping using newspaper')\n",
    "    articles_newspaper = crawler_newspaper(url, max_depth=max_depth)\n",
    "\n",
    "    to_remove = []\n",
    "    for url, art in tqdm(articles_newspaper.items()):\n",
    "        if url in articles_urls:\n",
    "            to_remove.append(url)\n",
    "            continue\n",
    "        try:\n",
    "            art.download()\n",
    "            art.parse()\n",
    "            art.nlp()\n",
    "        except:\n",
    "            to_remove.append(url)\n",
    "    for u in to_remove:\n",
    "        del articles_newspaper[u]\n",
    "\n",
    "    all_articles = dict(articles_urls)\n",
    "    all_articles.update(articles_newspaper)\n",
    "\n",
    "    all_articles = {u: a for u, a in all_articles.items() if a.text != ''}\n",
    "\n",
    "    print('scraped %d articles' % len(all_articles))\n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler_newspaper(root_url, max_depth=5):\n",
    "    all_articles = {}\n",
    "    my_domain = get_domain(root_url)\n",
    "    def crawl(url, my_domain, all_articles, depth):\n",
    "        \n",
    "        if depth > max_depth:\n",
    "            return\n",
    "        newspaper_obj = newspaper.build(url, config=newspaper_config, request_timeout=8, number_threads=4)\n",
    "        \n",
    "        new_dict = dict(zip(newspaper_obj.article_urls(), newspaper_obj.articles))\n",
    "        new_dict = {u: a for u, a in new_dict.items() if get_domain(url) == my_domain}\n",
    "        new_keys = set(new_dict.keys()).difference(all_articles)\n",
    "        print(url, len(newspaper_obj.articles), len(new_keys))\n",
    "#         if depth >= 1:\n",
    "#             print(new_keys)\n",
    "        \n",
    "        all_articles.update(new_dict)\n",
    "#         print(url, len(newspaper_obj.articles))\n",
    "        for u in newspaper_obj.article_urls():\n",
    "            if u in new_keys:\n",
    "                crawl(u, my_domain, all_articles, depth+1)\n",
    "                \n",
    "        \n",
    "    crawl(root_url,my_domain, all_articles, 0)\n",
    "    \n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = {\n",
    "    \n",
    "    'Unicef': 'https://www.unicef.org/coronavirus/covid-19',\n",
    "    'DWB':'https://www.doctorswithoutborders.org/facts-and-figures-about-coronavirus-disease-outbreak-covid-19',\n",
    "    'NIH': 'https://www.nih.gov/health-information/coronavirus',\n",
    "    'cov gov': 'https://www.coronavirus.gov/',\n",
    "    'CDC': 'https://www.cdc.gov/coronavirus/2019-ncov/index.html',\n",
    "    'cov uk': 'https://www.gov.uk/coronavirus',\n",
    "    'WHO': 'https://www.who.int/emergencies/diseases/novel-coronavirus-2019',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "for name, url in urls.items():\n",
    "    fn = name+'.pkl'\n",
    "    if os.path.isfile(fn):\n",
    "        continue\n",
    "    all_articles = get_all_articles(url, max_depth=3)\n",
    "    results[name] = all_articles\n",
    "    all_article_data = {url: get_article_data(art) for url, art in all_articles.items()}\n",
    "    with open(fn, 'wb') as f:\n",
    "        pickle.dump(all_article_data, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in all_articles.values():\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, all_articles in results.items():\n",
    "    fn = name+'.pkl'\n",
    "    all_article_data = {url: get_article_data(art) for url, art in all_articles.items() if art.meta_lang in ['', 'en']}\n",
    "    with open(fn, 'wb') as f:\n",
    "        pickle.dump(all_article_data, f, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unicef\n",
      "[('en', 208), ('fr', 39), ('es', 31), ('zh', 29), ('ar', 27), ('', 7)]\n",
      "DWB\n",
      "[('en', 31)]\n",
      "NIH\n",
      "[('en', 253), ('', 33), ('pl', 7), ('es', 5)]\n",
      "cov gov\n",
      "[('en', 16)]\n",
      "CDC\n",
      "[('en', 417), ('es', 144), ('', 4), ('zh', 1)]\n",
      "cov uk\n",
      "[('en', 407)]\n",
      "WHO\n",
      "[('en', 440), ('', 142)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "for name, all_articles in results.items():\n",
    "    langs = [a.meta_lang for a in all_articles.values()]\n",
    "    c = Counter(langs)\n",
    "    print(name)\n",
    "    print(c.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_data(article):\n",
    "    article_data = {}\n",
    "    article.url = article.url.strip()\n",
    "    article_data['title'] = article.title\n",
    "    article_data['text'] = article.text\n",
    "    article_data['url'] = article.url.strip()\n",
    "    article_data['keywords'] = article.keywords\n",
    "    article_data['summary'] = article.summary\n",
    "    article_data['meta_lang'] = article.meta_lang\n",
    "    article_data['meta_keywords'] = article.meta_keywords\n",
    "    return article_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwds = ['covid19', 'coronavirus', 'wuhan', 'sars', 'mers', 'covid-19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.unicef.org/coronavirus/covid-19\n",
      "https://www.unicef.org/innovation/stories/UReportCoronavirusIndonesia\n",
      "https://www.unicef.org/appeals/covid-2019.html\n",
      "http://www.unicef.org/wash\n",
      "https://www.unicef.org/stories/novel-coronavirus-outbreak-what-parents-should-know\n",
      "https://www.unicef.org/coronavirus/how-talk-your-child-about-coronavirus-covid-19\n",
      "https://www.unicef.org/coronavirus/6-ways-parents-can-support-their-kids-through-coronavirus-covid-19\n",
      "https://www.unicef.org/coronavirus/how-teenagers-can-protect-their-mental-health-during-coronavirus-covid-19\n",
      "https://www.unicef.org/press-releases/covid-19-children-heightened-risk-abuse-neglect-exploitation-and-violence-amidst\n",
      "https://www.unicef.org/coronavirus/everything-you-need-know-about-washing-your-hands-protect-against-coronavirus-covid-19\n",
      "https://www.unicef.org/coronavirus/how-teachers-can-talk-children-about-coronavirus-disease-covid-19\n",
      "https://www.unicef.org/press-releases/un-releases-15-million-help-vulnerable-countries-battle-spread-coronavirus\n",
      "https://www.voicesofyouth.org/campaign/studying-home-due-coronavirus-how-young-people-around-world-are-keeping-their-mood\n",
      "https://www.voicesofyouth.org/covid-19-your-voices-against-stigma-and-discrimination\n",
      "https://www.unicef.cn/en/what-we-do/unicef-emergencies/covid-19/five-year-old-wuhan-gets-help-while-family-isolated-treatment\n",
      "https://www.unicef.org/reports/key-messages-and-actions-coronavirus-disease-covid-19-prevention-and-control-schools\n",
      "https://www.unicef.org/press-releases/fact-sheet-handwashing-soap-critical-fight-against-coronavirus-out-reach-billions\n",
      "https://www.unicef.cn/press-releases/fact-sheet-handwashing-soap-critical-fight-against-coronavirus-out-reach-billions\n",
      "https://www.unicef.org/press-releases/world-poetry-day-young-people-war-zones-across-world-share-heart-wrenching-poems\n",
      "https://www.unicef.org/press-releases/covid-19-does-not-discriminate-nor-should-our-response\n",
      "http://www.unicef.org/press-releases/covid-19-children-heightened-risk-abuse-neglect-exploitation-and-violence-amidst\n",
      "http://www.unicef.org/coronavirus/6-ways-parents-can-support-their-kids-through-coronavirus-covid-19\n",
      "http://www.unicef.org/coronavirus/everything-you-need-know-about-washing-your-hands-protect-against-coronavirus-covid-19\n",
      "https://www.unicef.org/stories/what-are-we-waiting-for-obesity-mexico\n"
     ]
    }
   ],
   "source": [
    "for u, art in all_articles.items():\n",
    "    if len(art.text) > 200 and any([kw in art.text.lower() for kw in kwds]):\n",
    "        print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coronaton",
   "language": "python",
   "name": "coronaton"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
